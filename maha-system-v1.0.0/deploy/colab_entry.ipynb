{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfb5 Maha-System: The Indian AI Orchestra\n",
        "\n",
        "**Run India's sovereign AI models on free Google Colab T4 GPU**\n",
        "\n",
        "This notebook implements the \"Jugaad\" architecture from the manifesto:\n",
        "- Sequential model loading (hot-swap) to fit in 16GB VRAM\n",
        "- Translate-Reason-Verify (TRV) pipeline for SOTA reasoning\n",
        "- Cultural contextualization for Indian languages\n",
        "\n",
        "\ud83d\udcc4 [Read the Manifesto](https://github.com/yourusername/maha-system)\n",
        "\n",
        "\u26a0\ufe0f **Requirements**: GPU Runtime (Runtime \u2192 Change runtime type \u2192 T4 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 1. Setup: Install Dependencies\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --quiet\n",
        "!pip install huggingface_hub langgraph pyyaml --quiet\n",
        "\n",
        "print(\"\u2705 Dependencies installed\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 2. Mount Google Drive (Optional but Recommended)\n",
        "# Models are large (~15GB total). Storing in Drive avoids re-downloading.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create symlink to Drive for persistent storage\n",
        "MODEL_DIR = \"/content/drive/MyDrive/maha-system/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "!ln -sf {MODEL_DIR} /content/models\n",
        "\n",
        "print(f\"\ud83d\udcc1 Models will be stored in: {MODEL_DIR}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 3. Download Models (One-time)\n",
        "# Downloads ~15GB of models. Takes 5-10 minutes depending on connection.\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "models_to_download = [\n",
        "    {\n",
        "        \"name\": \"Sarvam-1 (2B) - Translator\",\n",
        "        \"repo_id\": \"sarvamai/sarvam-1\",\n",
        "        \"filename\": \"sarvam-1-2b-q4.gguf\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"DeepSeek-R1 (8B) - Reasoner\",\n",
        "        \"repo_id\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B-GGUF\",\n",
        "        \"filename\": \"DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Airavata (7B) - Critic\",\n",
        "        \"repo_id\": \"ai4bharat/airavata\",\n",
        "        \"filename\": \"airavata-7b-q4.gguf\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in models_to_download:\n",
        "    print(f\"\u2b07\ufe0f Downloading {model['name']}...\")\n",
        "    try:\n",
        "        path = hf_hub_download(\n",
        "            repo_id=model[\"repo_id\"],\n",
        "            filename=model[\"filename\"],\n",
        "            local_dir=\"/content/models\",\n",
        "            local_dir_use_symlinks=False,\n",
        "            resume_download=True\n",
        "        )\n",
        "        size = os.path.getsize(path) / (1024**3)\n",
        "        print(f\"   \u2705 {size:.2f}GB - {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Error: {e}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfb5 Orchestra assembled!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 4. Clone Maha-System Repository\n",
        "!git clone https://github.com/yourusername/maha-system.git /content/maha-system\n",
        "%cd /content/maha-system\n",
        "!pip install -e . --quiet"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 5. Test VRAM Management\n",
        "import torch\n",
        "from maha_system.core import VRAMManager\n",
        "\n",
        "print(\"GPU Status:\")\n",
        "print(f\"  Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
        "    stats = VRAMManager.get_memory_stats()\n",
        "    print(f\"  Total VRAM: {stats['total_gb']:.2f} GB\")\n",
        "    print(f\"  Currently allocated: {stats['allocated_gb']:.2f} GB\")\n",
        "\n",
        "# Test flush protocol\n",
        "VRAMManager.flush()\n",
        "print(\"\\n\u2705 VRAM flush protocol working\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 6. Run Maha-System Demo\n",
        "from maha_system.core import JugaadOrchestrator, TRVPipeline\n",
        "import yaml\n",
        "\n",
        "# Configuration\n",
        "MODEL_PATHS = {\n",
        "    \"translator\": \"/content/models/sarvam-1-2b-q4.gguf\",\n",
        "    \"reasoner\": \"/content/models/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\",\n",
        "    \"critic\": \"/content/models/airavata-7b-q4.gguf\"\n",
        "}\n",
        "\n",
        "# Load prompts\n",
        "with open(\"prompts/meta_prompts.yaml\") as f:\n",
        "    PROMPTS = yaml.safe_load(f)\n",
        "\n",
        "# Initialize\n",
        "orchestrator = JugaadOrchestrator(MODEL_PATHS)\n",
        "pipeline = TRVPipeline(orchestrator, PROMPTS)\n",
        "\n",
        "# Test query (Hindi riddle)\n",
        "TEST_QUERY = \"\u090f\u0915 \u0930\u0938\u094d\u0938\u0940 \u0915\u0940 \u0926\u094b \u091f\u0941\u0915\u0921\u093c\u0947, \u0926\u094b\u0928\u094b\u0902 \u0915\u0947 \u0926\u094b\u0928\u094b\u0902 \u0930\u0942\u0916\u0947\u0964 \u0907\u0938\u0915\u093e \u092e\u0924\u0932\u092c \u0915\u094d\u092f\u093e \u0939\u0948?\"\n",
        "\n",
        "print(\"\ud83e\uddea Running TRV Pipeline on Hindi riddle...\")\n",
        "print(f\"Query: {TEST_QUERY}\\n\")\n",
        "\n",
        "result = pipeline.execute(\n",
        "    query=TEST_QUERY,\n",
        "    language=\"hindi\",\n",
        "    enable_critic=True\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Answer: {result['final_answer']}\")\n",
        "print(f\"\\nIterations: {result.get('iterations', 0)}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# @title 7. Interactive Mode (Run this cell multiple times)\n",
        "query = \"\"\"\u090f\u0915 \u0930\u0938\u094d\u0938\u0940 \u0915\u0940 \u0926\u094b \u091f\u0941\u0915\u0921\u093c\u0947, \u0926\u094b\u0928\u094b\u0902 \u0915\u0947 \u0926\u094b\u0928\u094b\u0902 \u0930\u0942\u0916\u0947\"\"\" #@param {type:\"string\"}\n",
        "language = \"hindi\" #@param [\"hindi\", \"tamil\", \"telugu\", \"hinglish\"]\n",
        "enable_critic = True #@param {type:\"boolean\"}\n",
        "show_reasoning = False #@param {type:\"boolean\"}\n",
        "\n",
        "result = pipeline.execute(\n",
        "    query=query,\n",
        "    language=language,\n",
        "    enable_critic=enable_critic\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udcdd Answer:\")\n",
        "print(result['final_answer'])\n",
        "\n",
        "if show_reasoning:\n",
        "    print(f\"\\n\ud83d\udd0d Reasoning Trace:\")\n",
        "    for step in result['reasoning_trace']:\n",
        "        print(f\"\\n{step['phase'].upper()}:\")\n",
        "        print(step['output'][:300] + \"...\" if len(step['output']) > 300 else step['output'])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udca1 Tips for Best Results\n",
        "\n",
        "1. **VRAM Management**: If you get OOM errors, restart runtime (Ctrl+M) and run cells 1-5 again\n",
        "2. **Model Hot-swap**: The system unloads models after each use. First query is slow (loading), subsequent are faster.\n",
        "3. **Critic Phase**: Disable for faster responses, enable for higher accuracy on complex reasoning\n",
        "4. **Language Support**: Best for Hindi/Tamil. For Hinglish, use `language='hinglish'` and the Bridge model.\n",
        "\n",
        "## \ud83d\udcca Benchmark Results\n",
        "\n",
        "Expected performance on T4 GPU:\n",
        "- **Sarvam-1 (2B)**: 1.5GB VRAM, 50 tokens/sec\n",
        "- **DeepSeek-R1 (8B)**: 5GB VRAM, 25 tokens/sec\n",
        "- **Total Pipeline**: ~30-60 seconds per complex query (including hot-swap overhead)"
      ]
    }
  ]
}